<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>ERGO-12</title>
        <link rel="stylesheet" href="../../css/w3.css">
        <link rel="stylesheet"
              href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
              integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk"
              crossorigin="anonymous">

    <script src="https://kit.fontawesome.com/8c9171bec9.js" crossorigin="anonymous"></script>
    <link rel="icon" href="/assets/Portfolio Icon.jfif">

        <meta name="keywords" content="avatar, head avatars, computer vision, paper, research"/>
    </head>
    <body>
        <br/>
        <br/>
        <div class="w3-container" id="paper">
            <div class="w3-content" style="max-width:850px">
                <h2 align="center" id="title"><b>From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection</b></h2>
                <br/>

                <p align="center" class="center_text" id="authors">
                    <a target="_blank" href="https://scholar.google.com/citations?hl=en&user=bUEKG24AAAAJ">Nikola ZubiÄ‡*</a><sup>1</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://scholar.google.com/citations?user=FWpgbBsAAAAJ&hl=en">Daniel Gehrig*</a><sup>1</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://scholar.google.com/citations?user=uTMjaVoAAAAJ&hl=en">Mathias Gehrig</a><sup>1</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://scholar.google.com/citations?user=SC9wV2kAAAAJ&hl=en">Davide Scaramuzza</a><sup>1</sup>
                    &nbsp;&nbsp;
                </p>
    
                <p class="center_text" align="center">
                    * denotes equal contribution
                    &nbsp;&nbsp;<br>
                    &nbsp;&nbsp;<br>
                    
                    <sup>1</sup>Robotics and Perception Group, University of Zurich, Switzerland
                    &nbsp;&nbsp;<br>
                </p>
                <br>
                <p align="center">
                    <a href="https://arxiv.org/abs/2304.13455" target="__blank" class="btn btn-outline-primary btn-lg mx-3"><i class="fa-regular fa-file-pdf"></i>  Paper</a>

                    <a href="https://github.com/uzh-rpg/event_representation_study" target="__blank" class="btn btn-outline-primary btn-lg mx-3"><i class="fa-brands fa-github fa-l" data-tippy-content="GitHub"></i>  Code</a>
                </p>
                <br>

                <h3 class="w3-left-align" id="intro"><b>Abstract</b></h3>
                <p class="w3-justify">
                    Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. This work eliminates this bottleneck by selecting representations based on the Gromov-Wasserstein Discrepancy (GWD) between raw events and their representation. It is about 200 times faster to compute than training a neural network and preserves the task performance ranking of event representations across multiple representations, network backbones, datasets, and tasks. Thus finding representations with high task scores is equivalent to finding representations with a low GWD. We use this insight to, for the first time, perform a hyperparameter search on a large family of event representations, revealing new and powerful representations that exceed the state-of-the-art. Our optimized representations outperform existing representations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1 dataset, two established object detection benchmarks, and reach a 3.8% higher classification score on the mini N-ImageNet benchmark. Moreover, we outperform state-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methods by 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field of explicit representation optimization for event-based learning.
                </p>
                <br>           
                <h3 class="w3-left-align" id="cite"><b>Cite</b></h3>
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 12px">                
@InProceedings{Zubic_2023_ICCV,
    author    = {Zubi\'c, Nikola and Gehrig, Daniel and Gehrig, Mathias and Scaramuzza, Davide},
    title     = {From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {12846-12856}
}           
                </pre>
            </div>
        </div>
        <br/>
        <br/>
    </body>
</html>
