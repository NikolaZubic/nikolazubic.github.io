<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>DINER</title>
        <link rel="stylesheet" href="../../css/w3.css">
        <link rel="stylesheet"
              href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
              integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk"
              crossorigin="anonymous">

    <script src="https://kit.fontawesome.com/8c9171bec9.js" crossorigin="anonymous"></script>
    <link rel="icon" href="/assets/Portfolio Icon.jfif">

        <meta name="keywords" content="avatar, head avatars, computer vision, paper, research"/>
    </head>
    <body>
        <br/>
        <br/>
        <div class="w3-container" id="paper">
            <div class="w3-content" style="max-width:850px">
                <h2 align="center" id="title"><b>DINER: Depth-aware Image-based NEural Radiance Fields</b></h2>
                <br/>

                <p align="center" class="center_text" id="authors">
                    <a target="_blank" href="https://de.linkedin.com/in/malte-prinzler">Malte Prinzler</a><sup>1,3</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://ait.ethz.ch/people/hilliges/">Otmar Hilliges</a><sup>2</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://justusthies.github.io/">Justus Thies</a><sup>1</sup>
                    &nbsp;&nbsp;
                </p>

                <p class="center_text" align="center">
                    <sup>1</sup>Max Planck Institute for Intelligent Systems, Tübingen, Germany
                    &nbsp;&nbsp;<br>
                    <sup>2</sup>ETH Zürich
                    &nbsp;&nbsp;<br>
                    <sup>3</sup>Max Planck ETH Center for Learning Systems
                </p>
                <br>
                <p align="center">
                    <a href="https://arxiv.org/abs/2211.16630" target="__blank" class="btn btn-outline-primary btn-lg mx-3"><i class="fa-regular fa-file-pdf"></i>  Paper</a>

                    <a href="https://github.com/malteprinzler/diner" target="__blank" class="btn btn-outline-primary btn-lg mx-3"><i class="fa-brands fa-github fa-l" data-tippy-content="GitHub"></i>  Code</a>
                </p>
                <br>
                <br>
                <h3 class="w3-left-align" id="intro"><b>Video</b></h3>
                <p>
                    <video width="850" height="480" controls>
                      <source src="DINER_YOUTUBE.mp4" type="video/mp4">
                    </video>
<!--                     <iframe width="850" height="480" src="DINER_YOUTUBE.mp4" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
                    
                    <!-- <iframe width="850" height="480" src="https://www.youtube.com/embed/I17GbCCoytk" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
                <p/>

<!--                <p class="center_text font-weight-bold" align="center">
                    Accepted to CVPR 2022
                </p>
-->
                <br>
                <br>

                <h3 class="w3-left-align" id="intro"><b>Abstract</b></h3>
                <p class="w3-justify">
We present Depth-aware Image-based NEural Radiance fields (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to render 3D objects under novel views. Specifically, we propose novel techniques to incorporate depth information into feature fusion and efficient scene sampling. In comparison to the previous state of the art, DINER achieves higher synthesis quality and can process input views with greater disparity. This allows us to capture scenes more completely without changing capturing hardware requirements and ultimately enables larger viewpoint changes during novel view synthesis. We evaluate our method by synthesizing novel views, both for human heads and for general objects, and observe significantly improved qualitative results and increased perceptual metrics compared to the previous state of the art. The code will be made publicly available for research purposes.
                </p>
                <br>

                <h3 class="w3-left-align" id="publication"><b>Paper</b></h3>
                (<a href="DINER_arxiv.pdf" target="__blank">PDF</a>)
                <center>
                    <a href="DINER_arxiv.pdf" target="__blank"><img src="preview.jpg" style="max-width:80%" /></a>
                </center><br>               
                <h3 class="w3-left-align" id="cite"><b>Cite</b></h3>
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 12px">                
            @inproceedings{prinzler2022diner,
                title={DINER: Depth-aware Image-based NEural Radiance Fields},
                author={Prinzler, Malte and Hilliges, Otmar and Thies, Justus},
                booktitle = {Computer Vision and Pattern Recognition (CVPR)},
                year = {2023}
            }            
                </pre>
            </div>
        </div>
        <br/>
        <br/>
    </body>
</html>
